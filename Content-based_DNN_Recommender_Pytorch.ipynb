{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to recommend an article for a visitor to the site. We use the information of current content -- content ID, category (news, lifestyle, etc.), title, author, date, to predict the next content for the user. Note that the visitor ID is ignored in this task, meaning the model does not learn browse history for a particular visitor. Just infer the next content based on the current content.\n",
    "\n",
    "The dataset is pulled from the publicly avialable Kurier.at dataset in BigQuery using SQL queries. Kurier.at is an Austrian newsite. The data have already separated to training set (179,092 records) and test set (25,599 records, some records may belong to the same visitor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            visitor_id  content_id        category  \\\n",
      "0  1093134947164137327   299801977            News   \n",
      "1  1110195149925330322   299844825            News   \n",
      "2  1110195149925330322   299833903            News   \n",
      "3  1114140322257389822   299425707  Stars & Kultur   \n",
      "4  1114140322257389822   299775313  Stars & Kultur   \n",
      "\n",
      "                                               title               author  \\\n",
      "0       Kanzlerin Merkel strebt stabile Regierung an          Peter Temel   \n",
      "1  Regierungsbildung: SPD und CDU bringen sich in...  Sandra Lumetsberger   \n",
      "2      Ungarns Regierung k√§mpft mit Buch gegen Soros          Peter Temel   \n",
      "3  Angelina Jolie: Sie fordert mehr Geld von Brad...    Elisabeth Spitzer   \n",
      "4  Alexander Newley Sohn von Joan Collins: \"Mein ...   Christina Michlits   \n",
      "\n",
      "   month_since_epoch  next_content_id  \n",
      "0              574.0        299816215  \n",
      "1              574.0        299866366  \n",
      "2              574.0        298972803  \n",
      "3              574.0        299775313  \n",
      "4              574.0        299765217  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "train = pd.read_csv('training_set.csv', names = ['visitor_id', 'content_id', 'category', 'title', 'author', 'month_since_epoch', 'next_content_id'])\n",
    "test = pd.read_csv('test_set.csv', names = ['visitor_id', 'content_id', 'category', 'title', 'author', 'month_since_epoch', 'next_content_id'])\n",
    "\n",
    "train['next_content_id'] = train.next_content_id.astype('Int64')\n",
    "test['next_content_id'] = test.next_content_id.astype('Int64')\n",
    "\n",
    "print(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The month_since_epoch attribute is derived by (browse date - 1/1/1970), assuming 1/1/1970 is the date that the website was created. Only 'category' and 'author' attributes have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values of training set:\n",
      "visitor_id               0\n",
      "content_id               0\n",
      "category              1440\n",
      "title                    0\n",
      "author               32577\n",
      "month_since_epoch        1\n",
      "next_content_id          1\n",
      "dtype: int64\n",
      "Missing values of test set:\n",
      "visitor_id              0\n",
      "content_id              0\n",
      "category              165\n",
      "title                   0\n",
      "author               4649\n",
      "month_since_epoch       0\n",
      "next_content_id         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Missing values of training set:')\n",
    "print(train.isnull().sum())\n",
    "print('Missing values of test set:')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Convert 'content_id' and 'author' to integers (i.e. 1,2,3,...). Note that the vocabulary of 'content_id' and 'author' are comprised by both training and test data (not only training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            visitor_id  content_id category  \\\n",
      "0  1093134947164137327        3320     News   \n",
      "1  1110195149925330322        3356     News   \n",
      "\n",
      "                                               title  author  \\\n",
      "0       Kanzlerin Merkel strebt stabile Regierung an      28   \n",
      "1  Regierungsbildung: SPD und CDU bringen sich in...       4   \n",
      "\n",
      "   month_since_epoch  next_content_id  \n",
      "0              574.0             3104  \n",
      "1              574.0             3108  \n"
     ]
    }
   ],
   "source": [
    "def get_list(col):\n",
    "    '''Get the list of unique values of a column.\n",
    "    \n",
    "    Argument:\n",
    "    -- col: dataset.column\n",
    "    \n",
    "    Return:\n",
    "    -- A list of unique values.'''\n",
    "\n",
    "    lst = list(col.unique())\n",
    "    \n",
    "    return lst\n",
    "\n",
    "\n",
    "def map_to_id(col):\n",
    "    ''' Create a dictionary maps the content_id/author into index (0, 1, 2, ...).\n",
    "    \n",
    "    Argument:\n",
    "    -- col: a list of dataset.column, e.g. [dataset.content_id, dataset.next_content_id]\n",
    "    \n",
    "    Return:\n",
    "    -- A dictionary, e.g. (for content_id) {299801977: 0, ...}\n",
    "    -- size: the size of unique values. '''\n",
    "    \n",
    "    unique_lst = []\n",
    "    for e in col:\n",
    "        e_lst = get_list(e)\n",
    "        unique_lst.extend(e_lst)\n",
    "    size = len(unique_lst)\n",
    "    dct = dict()\n",
    "    i = 0\n",
    "    while i < len(unique_lst):\n",
    "        dct[unique_lst[i]] = i\n",
    "        i += 1\n",
    "        \n",
    "    return dct, size\n",
    "\n",
    "\n",
    "def contentID_author_preprocessing(train, test):\n",
    "    ''' Preprocess the content_id and author columns.\n",
    "    Convert them to integers from 0, 1, ... to V, where V is the number of unique values'''\n",
    "    \n",
    "    full_dataset = pd.concat([test,train], axis=0)\n",
    "    author_dct, author_size = map_to_id([full_dataset.author])\n",
    "    contentID_dct, content_size = map_to_id([full_dataset.content_id, full_dataset.next_content_id])\n",
    "    \n",
    "    # change 'content_id' column to indices\n",
    "    train['content_id'] = train.content_id.apply(lambda x: contentID_dct[x])\n",
    "    test['content_id'] = test.content_id.apply(lambda x: contentID_dct[x])\n",
    "\n",
    "    # change 'next_content_id' column to indices\n",
    "    train['next_content_id'] = train.next_content_id.apply(lambda x: contentID_dct[x])\n",
    "    test['next_content_id'] = test.next_content_id.apply(lambda x: contentID_dct[x])\n",
    "\n",
    "    # change 'author' column to indices\n",
    "    train['author'] = train.author.apply(lambda x: author_dct[x])\n",
    "    test['author'] = test.author.apply(lambda x: author_dct[x])\n",
    "    return train, test, author_size, content_size\n",
    "    \n",
    "train, test, author_size, content_size = contentID_author_preprocessing(train, test)\n",
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct 'category_month' column. First bucketize 'month_since_epoch' to bucket boundaries (400, 420, 440, ..., 680, 700) (16 buckets). Then fill 'missing' in the missing cells of 'category' as a value. Finally, implement one-hot encoding on different combinations of 'month_since_epoch' (16 unique buckets) and 'category' (4 unique values).\n",
    "\n",
    "In these particular datasets, there are 18 unique values for 'category_month' in the training data. There are 14 unique values for 'category_month' in the test data, which is a subset of the 18 unique values of training set. So I fit the OneHotEncoder to the training data, and transform it on the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            visitor_id  content_id category  \\\n",
      "0  1093134947164137327        3320     News   \n",
      "1  1110195149925330322        3356     News   \n",
      "\n",
      "                                               title  author  \\\n",
      "0       Kanzlerin Merkel strebt stabile Regierung an      28   \n",
      "1  Regierungsbildung: SPD und CDU bringen sich in...       4   \n",
      "\n",
      "   month_since_epoch  next_content_id  category_month_Lifestyle15  \\\n",
      "0                  9             3104                         0.0   \n",
      "1                  9             3108                         0.0   \n",
      "\n",
      "   category_month_Lifestyle6  category_month_Lifestyle7  ...  \\\n",
      "0                        0.0                        0.0  ...   \n",
      "1                        0.0                        0.0  ...   \n",
      "\n",
      "   category_month_News8  category_month_News9  category_month_Stars & Kultur6  \\\n",
      "0                   0.0                   1.0                             0.0   \n",
      "1                   0.0                   1.0                             0.0   \n",
      "\n",
      "   category_month_Stars & Kultur7  category_month_Stars & Kultur8  \\\n",
      "0                             0.0                             0.0   \n",
      "1                             0.0                             0.0   \n",
      "\n",
      "   category_month_Stars & Kultur9  category_month_missing6  \\\n",
      "0                             0.0                      0.0   \n",
      "1                             0.0                      0.0   \n",
      "\n",
      "   category_month_missing7  category_month_missing8  category_month_missing9  \n",
      "0                      0.0                      0.0                      0.0  \n",
      "1                      0.0                      0.0                      0.0  \n",
      "\n",
      "[2 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def create_category_month(train, test):\n",
    "    '''Create a 'category_month' column.'''\n",
    "    \n",
    "    # fill 'missing' in the missing values\n",
    "    train['category'] = train['category'].fillna('missing')\n",
    "    test['category'] = test['category'].fillna('missing')\n",
    "\n",
    "    # categorize month_category\n",
    "    boundaries = torch.tensor(range(400,700,20))\n",
    "    train['month_since_epoch'] = torch.bucketize(torch.tensor(train['month_since_epoch']), boundaries)\n",
    "    test['month_since_epoch'] = torch.bucketize(torch.tensor(test['month_since_epoch']), boundaries)\n",
    "\n",
    "    train['category_month'] = train['category'] + train['month_since_epoch'].astype(str)\n",
    "    test['category_month'] = test['category'] + test['month_since_epoch'].astype(str)\n",
    "\n",
    "    ohc = OneHotEncoder()\n",
    "    ohe = ohc.fit(train.category_month.values.reshape(-1,1))\n",
    "\n",
    "    a = ohe.transform(train.category_month.values.reshape(-1,1)).toarray()\n",
    "    train = pd.concat([train, pd.DataFrame(a, columns = ['category_month_' + str(ohc.categories_[0][i]) for i in range(len(ohc.categories_[0]))])], axis=1)\n",
    "    train.drop(['category_month'], axis = 1,inplace=True)\n",
    "\n",
    "    b = ohe.transform(test.category_month.values.reshape(-1,1)).toarray()\n",
    "    test = pd.concat([test, pd.DataFrame(b)], axis=1)\n",
    "    test.drop(['category_month'], axis = 1,inplace=True)\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "train, test = create_category_month(train, test)\n",
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'title' attribute uses pretrained embedding. I use NNLM model for Genman (https://tfhub.dev/google/nnlm-de-dim50/2). Fifty dimension is enough for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "def title_embedding(train, test):\n",
    "    ''' Convert 'title' column to pretrained embeddings (50 dimensions). '''\n",
    "    \n",
    "    pretrained_emb = hub.load(\"https://tfhub.dev/google/nnlm-de-dim50/2\")\n",
    "    train['title'] = train['title'].apply(lambda x: pretrained_emb([x]).numpy().reshape((50,)))\n",
    "    test['title'] = test['title'].apply(lambda x: pretrained_emb([x]).numpy().reshape((50,)))\n",
    "\n",
    "    emb_col_names = []\n",
    "    for i in range(50):\n",
    "        emb_col_names.append('title_emb_' + str(i+1))\n",
    "\n",
    "    train[emb_col_names] = pd.DataFrame(train.title.tolist(), index= train.index)\n",
    "    test[emb_col_names] = pd.DataFrame(test.title.tolist(), index= test.index)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "train, test = title_embedding(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   content_id  author  next_content_id  category_month_Lifestyle15  \\\n",
      "0        3320      28             3104                         0.0   \n",
      "1        3356       4             3108                         0.0   \n",
      "\n",
      "   category_month_Lifestyle6  category_month_Lifestyle7  \\\n",
      "0                        0.0                        0.0   \n",
      "1                        0.0                        0.0   \n",
      "\n",
      "   category_month_Lifestyle8  category_month_Lifestyle9  category_month_News5  \\\n",
      "0                        0.0                        0.0                   0.0   \n",
      "1                        0.0                        0.0                   0.0   \n",
      "\n",
      "   category_month_News6  ...  title_emb_45  title_emb_46  title_emb_47  \\\n",
      "0                   0.0  ...      0.076455      0.075862     -0.197285   \n",
      "1                   0.0  ...     -0.106623      0.025269     -0.109285   \n",
      "\n",
      "   title_emb_48  title_emb_49  title_emb_50  category_Lifestyle  \\\n",
      "0      0.310615      0.024261     -0.149852                 0.0   \n",
      "1     -0.007621     -0.142904     -0.152314                 0.0   \n",
      "\n",
      "   category_News  category_Stars & Kultur  category_missing  \n",
      "0            1.0                      0.0               0.0  \n",
      "1            1.0                      0.0               0.0  \n",
      "\n",
      "[2 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "# apply OneHotEncoding to category\n",
    "def One_hot_encoding(dataset, attr_lst):\n",
    "    \n",
    "    ''' Perform One Hot Encoding on an attribute of a dataset.\n",
    "    Arguments:\n",
    "        -- dataset: the data set.\n",
    "        -- attr_lst: the list of attribute names, in string form. E.g. ['a', 'b']\n",
    "        \n",
    "    Output:\n",
    "        return the dataset with added encoded attributes. And the original attribute is dropped'''\n",
    "        \n",
    "    ohc = OneHotEncoder()\n",
    "    for attr in attr_lst:\n",
    "        ohe = ohc.fit_transform(dataset[attr].values.reshape(-1,1)).toarray()\n",
    "        dfOneHot = pd.DataFrame(ohe,columns = [attr + '_' + str(ohc.categories_[0][i]) for i in range(len(ohc.categories_[0]))])\n",
    "        dataset = pd.concat([dataset,dfOneHot], axis=1)\n",
    "        dataset.drop([attr], axis = 1,inplace=True)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "train = One_hot_encoding(train, ['category'])\n",
    "test = One_hot_encoding(test, ['category'])\n",
    "\n",
    "# drop unwanted columns\n",
    "train.drop(['title', 'visitor_id', 'month_since_epoch'], axis = 1,inplace=True)\n",
    "test.drop(['title', 'visitor_id', 'month_since_epoch'], axis = 1,inplace=True)\n",
    "\n",
    "print(train.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data set has 74 features ('content_id': 1, 'category': 4, 'title': 50, 'author': 1, 'category_month': 18) and 1 target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in train set: 74\n",
      "Number of features in test set: 74\n"
     ]
    }
   ],
   "source": [
    "print('Number of features in train set:', train.shape[1]-1)\n",
    "print('Number of features in test set:', test.shape[1]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Dataset and DataLoader for efficient model training in PyTorch. \n",
    "Batch size is set to 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: the dataset, in pandas DataFrame form.\n",
    "        \"\"\"\n",
    "        self.df = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Retrieve instance(s) from data.\n",
    "        \n",
    "        Argument:\n",
    "            -- idx: an integer of a list of integers.\n",
    "            \n",
    "        return: \n",
    "            -- A tuple storing (X_content, X_author, X_rest, y)\n",
    "        '''\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        X_content = self.df.iloc[idx, self.df.columns == 'content_id']\n",
    "        X_content = torch.tensor(X_content, dtype = torch.int64)\n",
    "        X_author = self.df.iloc[idx, self.df.columns == 'author']\n",
    "        X_author = torch.tensor(X_author, dtype = torch.int64)\n",
    "        X_rest = self.df.iloc[idx, 3:]\n",
    "        X_rest = torch.tensor(X_rest, dtype = torch.float)\n",
    "        y = self.df.iloc[idx, self.df.columns == 'next_content_id']\n",
    "        y = torch.tensor(y, dtype = torch.int64)\n",
    "        sample = (X_content, X_author, X_rest, y)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "train_set = MyDataset(train)    \n",
    "loader_train = DataLoader(train_set, batch_size=512, shuffle = True)  \n",
    "\n",
    "test_set = MyDataset(test)   \n",
    "loader_test = DataLoader(test_set, batch_size=512, shuffle = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the neural network model. \n",
    "\n",
    "First, the 'content_id' goes through an embedding layer of dimension 10 and 'author' goes through an embedding layer of dimension 3. \n",
    "\n",
    "Next, concatenate the embedded 'content_id', embedded 'author' and the rest features. \n",
    "\n",
    "Then go through 'Linear1 (200 hidden neurons) - BatchNorm - ReLu - Linear2 (100 hidden neurons) - BatchNorm - ReLu - Linear3 (50 hidden neurons) - BatchNorm - ReLu - Linear4 (number_of_classes neurons) - softmax'.\n",
    "\n",
    "Take negative logarithm as the loss function (cross entropy loss = softmax + negative logarithm). \n",
    "\n",
    "Use AdaGrad as the optimization method. AdaGrad is designed to solve the two problems:\n",
    "1. In high dimension problem, progress along \"steep\" directions is damped and \"flat\" is accelerated. \n",
    "2. The learning rate is slowed down over long time (when approaching close to the minima)\n",
    "\n",
    "Just use the default initialization method in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the target labels do not need embedding. Because softmax simply takes the class with highest logit value as the predicted label.\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self, content_size, author_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.content_emb =  nn.Embedding(content_size, 10) \n",
    "        self.author_emb =  nn.Embedding(author_size, 3)\n",
    "        self.comb = nn.Sequential(\n",
    "                nn.Linear(85, 200),\n",
    "                nn.BatchNorm1d(200),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(200, 100),\n",
    "                nn.BatchNorm1d(100),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(100, 50),\n",
    "                nn.BatchNorm1d(50),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(50, content_size)\n",
    "                )\n",
    "\n",
    "    def forward(self, X_content, X_author, X_rest):        # X_rest size ([512, 72])                  \n",
    "        content = self.content_emb(X_content)              # torch.Size([512, 1, 10])\n",
    "        content = content.squeeze()\n",
    "        author = self.author_emb(X_author)                 # torch.Size([512, 1, 3])\n",
    "        author = author.squeeze()\n",
    "        output = torch.cat((content, author, X_rest), 1)   # torch.Size([512, 85])\n",
    "        output = self.comb(output)                         # torch.Size([512, 6326])\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "net = MyNet(content_size, author_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(net.parameters(), lr=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model. \n",
    "For implicity, the process of tuning parameter is skipped in this notebook. Just set the learning rate to 0.1 and fix the NN structure. Run for 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA \n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "net = net.to(device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 5.383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function CapturableResourceDeleter.__del__ at 0x000001E5D145E4C8>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py\", line 202, in __del__\n",
      "    self._destroy_resource()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 780, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 823, in _call\n",
      "    self._initialize(args, kwds, add_initializers_to=initializers)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 697, in _initialize\n",
      "    *args, **kwds))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 2855, in _get_concrete_function_internal_garbage_collected\n",
      "    graph_function, _, _ = self._maybe_define_function(args, kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3213, in _maybe_define_function\n",
      "    graph_function = self._create_graph_function(args, kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 3075, in _create_graph_function\n",
      "    capture_by_value=self._capture_by_value),\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\", line 986, in func_graph_from_py_func\n",
      "    func_outputs = python_func(*func_args, **func_kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\", line 600, in wrapped_fn\n",
      "    return weak_wrapped_fn().__wrapped__(*args, **kwds)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 237, in restored_function_body\n",
      "    return _call_concrete_function(function, inputs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\function_deserialization.py\", line 74, in _call_concrete_function\n",
      "    result = function._call_flat(tensor_inputs, function._captured_inputs)  # pylint: disable=protected-access\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py\", line 106, in _call_flat\n",
      "    cancellation_manager)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 1938, in _call_flat\n",
      "    flat_outputs = forward_function.call(ctx, args_with_tangents)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 579, in call\n",
      "    executor_type=executor_type)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 1192, in partitioned_call\n",
      "    f.add_to_graph(graph)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\", line 495, in add_to_graph\n",
      "    g._add_function(self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3345, in _add_function\n",
      "    gradient)\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: 'func' argument to TF_GraphCopyFunction cannot be null\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 5.204\n",
      "[1,   150] loss: 5.113\n",
      "[1,   200] loss: 5.065\n",
      "[1,   250] loss: 5.021\n",
      "[1,   300] loss: 4.989\n",
      "[1,   350] loss: 4.969\n",
      "[2,    50] loss: 4.740\n",
      "[2,   100] loss: 4.737\n",
      "[2,   150] loss: 4.740\n",
      "[2,   200] loss: 4.738\n",
      "[2,   250] loss: 4.735\n",
      "[2,   300] loss: 4.732\n",
      "[2,   350] loss: 4.729\n",
      "[3,    50] loss: 4.668\n",
      "[3,   100] loss: 4.665\n",
      "[3,   150] loss: 4.669\n",
      "[3,   200] loss: 4.670\n",
      "[3,   250] loss: 4.666\n",
      "[3,   300] loss: 4.666\n",
      "[3,   350] loss: 4.666\n",
      "[4,    50] loss: 4.610\n",
      "[4,   100] loss: 4.621\n",
      "[4,   150] loss: 4.618\n",
      "[4,   200] loss: 4.630\n",
      "[4,   250] loss: 4.631\n",
      "[4,   300] loss: 4.631\n",
      "[4,   350] loss: 4.629\n",
      "[5,    50] loss: 4.581\n",
      "[5,   100] loss: 4.592\n",
      "[5,   150] loss: 4.596\n",
      "[5,   200] loss: 4.598\n",
      "[5,   250] loss: 4.601\n",
      "[5,   300] loss: 4.602\n",
      "[5,   350] loss: 4.601\n",
      "[6,    50] loss: 4.580\n",
      "[6,   100] loss: 4.572\n",
      "[6,   150] loss: 4.572\n",
      "[6,   200] loss: 4.577\n",
      "[6,   250] loss: 4.581\n",
      "[6,   300] loss: 4.581\n",
      "[6,   350] loss: 4.581\n",
      "[7,    50] loss: 4.538\n",
      "[7,   100] loss: 4.551\n",
      "[7,   150] loss: 4.557\n",
      "[7,   200] loss: 4.557\n",
      "[7,   250] loss: 4.562\n",
      "[7,   300] loss: 4.563\n",
      "[7,   350] loss: 4.563\n",
      "[8,    50] loss: 4.543\n",
      "[8,   100] loss: 4.543\n",
      "[8,   150] loss: 4.545\n",
      "[8,   200] loss: 4.549\n",
      "[8,   250] loss: 4.549\n",
      "[8,   300] loss: 4.549\n",
      "[8,   350] loss: 4.548\n",
      "[9,    50] loss: 4.515\n",
      "[9,   100] loss: 4.522\n",
      "[9,   150] loss: 4.528\n",
      "[9,   200] loss: 4.530\n",
      "[9,   250] loss: 4.531\n",
      "[9,   300] loss: 4.534\n",
      "[9,   350] loss: 4.535\n",
      "[10,    50] loss: 4.505\n",
      "[10,   100] loss: 4.515\n",
      "[10,   150] loss: 4.516\n",
      "[10,   200] loss: 4.522\n",
      "[10,   250] loss: 4.524\n",
      "[10,   300] loss: 4.524\n",
      "[10,   350] loss: 4.523\n",
      "[11,    50] loss: 4.492\n",
      "[11,   100] loss: 4.502\n",
      "[11,   150] loss: 4.505\n",
      "[11,   200] loss: 4.506\n",
      "[11,   250] loss: 4.508\n",
      "[11,   300] loss: 4.512\n",
      "[11,   350] loss: 4.512\n",
      "[12,    50] loss: 4.499\n",
      "[12,   100] loss: 4.496\n",
      "[12,   150] loss: 4.494\n",
      "[12,   200] loss: 4.498\n",
      "[12,   250] loss: 4.500\n",
      "[12,   300] loss: 4.502\n",
      "[12,   350] loss: 4.502\n",
      "[13,    50] loss: 4.480\n",
      "[13,   100] loss: 4.488\n",
      "[13,   150] loss: 4.485\n",
      "[13,   200] loss: 4.482\n",
      "[13,   250] loss: 4.485\n",
      "[13,   300] loss: 4.489\n",
      "[13,   350] loss: 4.493\n",
      "[14,    50] loss: 4.489\n",
      "[14,   100] loss: 4.482\n",
      "[14,   150] loss: 4.482\n",
      "[14,   200] loss: 4.484\n",
      "[14,   250] loss: 4.486\n",
      "[14,   300] loss: 4.483\n",
      "[14,   350] loss: 4.483\n",
      "[15,    50] loss: 4.454\n",
      "[15,   100] loss: 4.457\n",
      "[15,   150] loss: 4.464\n",
      "[15,   200] loss: 4.472\n",
      "[15,   250] loss: 4.473\n",
      "[15,   300] loss: 4.475\n",
      "[15,   350] loss: 4.475\n",
      "[16,    50] loss: 4.458\n",
      "[16,   100] loss: 4.459\n",
      "[16,   150] loss: 4.460\n",
      "[16,   200] loss: 4.463\n",
      "[16,   250] loss: 4.463\n",
      "[16,   300] loss: 4.465\n",
      "[16,   350] loss: 4.467\n",
      "[17,    50] loss: 4.441\n",
      "[17,   100] loss: 4.446\n",
      "[17,   150] loss: 4.448\n",
      "[17,   200] loss: 4.449\n",
      "[17,   250] loss: 4.452\n",
      "[17,   300] loss: 4.457\n",
      "[17,   350] loss: 4.459\n",
      "[18,    50] loss: 4.428\n",
      "[18,   100] loss: 4.432\n",
      "[18,   150] loss: 4.439\n",
      "[18,   200] loss: 4.444\n",
      "[18,   250] loss: 4.446\n",
      "[18,   300] loss: 4.449\n",
      "[18,   350] loss: 4.452\n",
      "[19,    50] loss: 4.426\n",
      "[19,   100] loss: 4.436\n",
      "[19,   150] loss: 4.440\n",
      "[19,   200] loss: 4.444\n",
      "[19,   250] loss: 4.444\n",
      "[19,   300] loss: 4.444\n",
      "[19,   350] loss: 4.445\n",
      "[20,    50] loss: 4.427\n",
      "[20,   100] loss: 4.422\n",
      "[20,   150] loss: 4.433\n",
      "[20,   200] loss: 4.434\n",
      "[20,   250] loss: 4.436\n",
      "[20,   300] loss: 4.439\n",
      "[20,   350] loss: 4.439\n"
     ]
    }
   ],
   "source": [
    "max_epoch = 20\n",
    "\n",
    "for epoch in range(max_epoch):  \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(loader_train):    # 349 iterations per epoch\n",
    "        net.train()     \n",
    "\n",
    "        X_content, X_author, X_rest, labels = data\n",
    "        \n",
    "        X_content = X_content.to(device = device)\n",
    "        X_author = X_author.to(device = device)\n",
    "        X_rest = X_rest.to(device = device)\n",
    "        labels = labels.to(device = device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(X_content, X_author, X_rest)    # torch.Size([512, 6326])\n",
    "        loss = criterion(outputs, labels.squeeze())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / (i+1)))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 accuracy: 0.0632446579944529\n",
      "Top 10 accuracy: 0.3222000859408571\n"
     ]
    }
   ],
   "source": [
    "def get_topK_accuracy(net, loader_test, k):\n",
    "    \n",
    "    '''Return the top K accuracy.\n",
    "    Argument:\n",
    "    -- net: The trained model.\n",
    "    -- loader_test: The DataLoader of test set.\n",
    "    -- k: Top k, where k is an integer.\n",
    "    \n",
    "    Return: Top K accuracy. '''\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    net.eval()  \n",
    "    with torch.no_grad():\n",
    "        for X_content, X_author, X_rest, y in loader_test:\n",
    "            X_content = X_content.to(device=device)  # move to device, e.g. GPU\n",
    "            X_author = X_author.to(device=device)\n",
    "            X_rest = X_rest.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = net(X_content, X_author, X_rest)\n",
    "            _, preds = scores.topk(k=k, dim=1)     \n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        \n",
    "    return acc\n",
    "\n",
    "\n",
    "top1_acc = get_topK_accuracy(net, loader_test, 1)\n",
    "top10_acc = get_topK_accuracy(net, loader_test, 10)\n",
    "print('Top 1 accuracy:', top1_acc)\n",
    "print('Top 10 accuracy:', top10_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance.\n",
    "The accuracy is 0.06 and the top 10 accuracy is 0.32."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
